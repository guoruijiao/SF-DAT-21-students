{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SF-DAT-21 | Lab 05: Storms, `pandas`, and GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OJECTIVES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This session closes the course's first unit: \"Research Design and Data Analysis\" in which we covered important building blocks for performing data science work:\n",
    "- Research Design\n",
    "- Data Manipulation, dataset tidying and Exploratory Data Analysis with `pandas`\n",
    "- Statistics and Visualization with `pandas`\n",
    "\n",
    "Starting with the next session, we will shift our attention to building predictive models.  However, the skills you've learned so far will prove to be very fruitful as a data scientist (e.g., mastering tidying data will save you hours of time and make your data much easier to work on).  This lab gives you an opportunity to put into practice all these concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BACKGROUND"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Severe weather events cause public health and economic problems, many resulting in fatalities, injuries, and property/crop damage.  Preventing such outcomes to the extent possible is a key concern.\n",
    "\n",
    "Using the U.S. National Oceanic and Atmospheric Administration's (NOAA) storm database (in the `datasets` folder alongside some documentation), which tracked characteristics of major weather events in the United States from 1950 to 2011, you will study and report what types of events cause most of the fatalities, injuries, and economic damages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROMPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll work in group of 4 (no less) to 5 (no more), collaborate using git, GitHub, and the SF-DAT-21-students remote repository here: https://github.com/paspeur/SF-DAT-21-students\n",
    "\n",
    "Specifically:\n",
    "- Do individual work inside the directory named after your GitHub username.  This means copying this iPython Notebook (just this notebook `Storm.ipnb`) in your directory.  Commit and push your changes regularly.  (We'll go over it in class)\n",
    "- Name your team and have one person in your team to create a new directory at the root of the repository in the form `class-05-XXX`.  Copy over there this notebook.\n",
    "- How exactly you collaborate among yourself (as long as it works) is left for you to decide.  We are there to help you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DELIVERABLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Your team deliverable (more below) will be the notebook in your team directory; your individual deliverable will be the notebook in your personal directory (it can be the final team notebook but I'm mostly interested in seeing you building proficiency with git/GitHub and this is one way to demonstrate)\n",
    "- Your notebook should not just be code, not text either.  I'm looking for both text and code, one supporting the other in a logical manner.\n",
    "- More specifically, I want you to approach your work following the data science workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA SCIENCE WORKFLOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Don't read into each question too literally.  The lab is relatively open-ended; the goal is to improve the mastery of your new skills and have fun doing it.]\n",
    "\n",
    "- **1. IDENTIFY the Problem**\n",
    "  - Write a SMART research question around the vaguely following objective: \"What types of events result in most fatalities, injuries, and economic damages?\"\n",
    "\n",
    "\n",
    "- **2. ACQUIRE the Data**\n",
    "  - The raw dataset you'll work with is in the `datasets` folder (at the root of the repository) alongside some documentation\n",
    "  - Questions you might ask yourself:\n",
    "    - _What type of data is it?  (e.g., cross-sectional or longitudinal)_\n",
    "    - _How well was the data collected?_\n",
    "    - _Is there much missing data?_\n",
    "    - _Was the data collection instrument calibrated?_\n",
    "    - _Is the dataset aggregated?_\n",
    "    - _Do we need pre-aggregated data?_\n",
    "\n",
    "\n",
    "- **3. PARSE the Data**\n",
    "  - [Again, the documentation is in the `datasets` folder]\n",
    "  - You need to understand what you're working with\n",
    "  - To better understand your data\n",
    "    - _Create or review the data dictionary_\n",
    "    - _Perform exploratory surface analysis_\n",
    "    - _Describe data structure and information being collected_\n",
    "    - _Explore variables and data types_\n",
    "\n",
    "\n",
    "- **4. Mine the Data**\n",
    "  - Mine the Data\n",
    "  - Determine sampling methodology and sample data\n",
    "  - Format, clean, slice, and combine data in Python\n",
    "  - Create necessary derived columns from the data (new data)\n",
    "\n",
    "\n",
    "- **7. Present the Results**\n",
    "  - Present the Results\n",
    "  - Summarize findings with narrative, storytelling techniques\n",
    "  - Present limitations and assumptions of your analysis\n",
    "  - Identify follow up problems and questions for future analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOCUMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Some documentation for the dataset is in the `datasets` folder at the root of the repository\n",
    "- `pandas` documentation: http://pandas.pydata.org/pandas-docs/stable/\n",
    "- Slides from the previous sessions..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDEAS TO KEEP IN MIND"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The datset is big.  You might want to use a small subset to get things working before moving on to the entire dataset.\n",
    "2. Generate both tables and histograms to answer your questions.  That'll make it into a better presentation to your classmates.\n",
    "3. At some point in time, you may want to split tidying up the dataset and the exploratory data analysis into different notebooks (multiple well defined notebooks make it easier to collaborate).  You could save your cleaned up dataset into disk (it should be small at this point) and load it from the other notebook.\n",
    "4. If time allows, create a new tidy dataset reporting fatalities, injuries, and property/crop damage (min, max, median, Q1, Q3, mean, and variance) per calendar year and per state and save it as a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE TO GET YOU STARTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruijiao.guo/anaconda/envs/py27/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2723: DtypeWarning: Columns (2,9,10,11,12,16,17,27,28,29,30,35) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join('..', 'datasets', 'Storms.csv.bz2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE__</th>\n",
       "      <th>BGN_DATE</th>\n",
       "      <th>BGN_TIME</th>\n",
       "      <th>TIME_ZONE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>...</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>LATITUDE_E</th>\n",
       "      <th>LONGITUDE_</th>\n",
       "      <th>REMARKS</th>\n",
       "      <th>REFNUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4/18/1950 0:00:00</td>\n",
       "      <td>130</td>\n",
       "      <td>CST</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>8812</td>\n",
       "      <td>3051</td>\n",
       "      <td>8806</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4/18/1950 0:00:00</td>\n",
       "      <td>145</td>\n",
       "      <td>CST</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>8755</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2/20/1951 0:00:00</td>\n",
       "      <td>1600</td>\n",
       "      <td>CST</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>8742</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6/8/1951 0:00:00</td>\n",
       "      <td>900</td>\n",
       "      <td>CST</td>\n",
       "      <td>89</td>\n",
       "      <td>...</td>\n",
       "      <td>8626</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>11/15/1951 0:00:00</td>\n",
       "      <td>1500</td>\n",
       "      <td>CST</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>8642</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902292</th>\n",
       "      <td>56</td>\n",
       "      <td>11/30/2011 0:00:00</td>\n",
       "      <td>10:30:00 PM</td>\n",
       "      <td>MST</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>EPISODE NARRATIVE: A strong cold front moved s...</td>\n",
       "      <td>902293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902293</th>\n",
       "      <td>30</td>\n",
       "      <td>11/10/2011 0:00:00</td>\n",
       "      <td>02:48:00 PM</td>\n",
       "      <td>MST</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>EPISODE NARRATIVE: A strong westerly flow alof...</td>\n",
       "      <td>902294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902294</th>\n",
       "      <td>2</td>\n",
       "      <td>11/8/2011 0:00:00</td>\n",
       "      <td>02:58:00 PM</td>\n",
       "      <td>AKS</td>\n",
       "      <td>213</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>EPISODE NARRATIVE: A 960 mb low over the south...</td>\n",
       "      <td>902295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902295</th>\n",
       "      <td>2</td>\n",
       "      <td>11/9/2011 0:00:00</td>\n",
       "      <td>10:21:00 AM</td>\n",
       "      <td>AKS</td>\n",
       "      <td>202</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>EPISODE NARRATIVE: A 960 mb low over the south...</td>\n",
       "      <td>902296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902296</th>\n",
       "      <td>1</td>\n",
       "      <td>11/28/2011 0:00:00</td>\n",
       "      <td>08:00:00 PM</td>\n",
       "      <td>CST</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>EPISODE NARRATIVE: An intense upper level low ...</td>\n",
       "      <td>902297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>902297 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        STATE__            BGN_DATE     BGN_TIME TIME_ZONE  COUNTY   ...    \\\n",
       "0             1   4/18/1950 0:00:00          130       CST      97   ...     \n",
       "1             1   4/18/1950 0:00:00          145       CST       3   ...     \n",
       "2             1   2/20/1951 0:00:00         1600       CST      57   ...     \n",
       "3             1    6/8/1951 0:00:00          900       CST      89   ...     \n",
       "4             1  11/15/1951 0:00:00         1500       CST      43   ...     \n",
       "...         ...                 ...          ...       ...     ...   ...     \n",
       "902292       56  11/30/2011 0:00:00  10:30:00 PM       MST       7   ...     \n",
       "902293       30  11/10/2011 0:00:00  02:48:00 PM       MST       9   ...     \n",
       "902294        2   11/8/2011 0:00:00  02:58:00 PM       AKS     213   ...     \n",
       "902295        2   11/9/2011 0:00:00  10:21:00 AM       AKS     202   ...     \n",
       "902296        1  11/28/2011 0:00:00  08:00:00 PM       CST       6   ...     \n",
       "\n",
       "       LONGITUDE LATITUDE_E LONGITUDE_  \\\n",
       "0           8812       3051       8806   \n",
       "1           8755          0          0   \n",
       "2           8742          0          0   \n",
       "3           8626          0          0   \n",
       "4           8642          0          0   \n",
       "...          ...        ...        ...   \n",
       "902292         0          0          0   \n",
       "902293         0          0          0   \n",
       "902294         0          0          0   \n",
       "902295         0          0          0   \n",
       "902296         0          0          0   \n",
       "\n",
       "                                                  REMARKS  REFNUM  \n",
       "0                                                     NaN       1  \n",
       "1                                                     NaN       2  \n",
       "2                                                     NaN       3  \n",
       "3                                                     NaN       4  \n",
       "4                                                     NaN       5  \n",
       "...                                                   ...     ...  \n",
       "902292  EPISODE NARRATIVE: A strong cold front moved s...  902293  \n",
       "902293  EPISODE NARRATIVE: A strong westerly flow alof...  902294  \n",
       "902294  EPISODE NARRATIVE: A 960 mb low over the south...  902295  \n",
       "902295  EPISODE NARRATIVE: A 960 mb low over the south...  902296  \n",
       "902296  EPISODE NARRATIVE: An intense upper level low ...  902297  \n",
       "\n",
       "[902297 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ALL YOURS NOW..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(902297, 37)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'STATE__', u'BGN_DATE', u'BGN_TIME', u'TIME_ZONE', u'COUNTY',\n",
       "       u'COUNTYNAME', u'STATE', u'EVTYPE', u'BGN_RANGE', u'BGN_AZI',\n",
       "       u'BGN_LOCATI', u'END_DATE', u'END_TIME', u'COUNTY_END', u'COUNTYENDN',\n",
       "       u'END_RANGE', u'END_AZI', u'END_LOCATI', u'LENGTH', u'WIDTH', u'F',\n",
       "       u'MAG', u'FATALITIES', u'INJURIES', u'PROPDMG', u'PROPDMGEXP',\n",
       "       u'CROPDMG', u'CROPDMGEXP', u'WFO', u'STATEOFFIC', u'ZONENAMES',\n",
       "       u'LATITUDE', u'LONGITUDE', u'LATITUDE_E', u'LONGITUDE_', u'REMARKS',\n",
       "       u'REFNUM'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140528.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.INJURIES.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15145.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.FATALITIES.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "902297\n",
      "902297\n"
     ]
    }
   ],
   "source": [
    "print len(df.FATALITIES)\n",
    "print df.FATALITIES.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
